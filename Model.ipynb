{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8407d0ac-057a-4da0-b99e-69b06ea93ed7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from joblib import parallel_backend\n",
    "import xgboost as xgb\n",
    "from functools import partial\n",
    "from lightgbm import LGBMClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import time\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a733022-519d-4f5d-b66a-8c7ab8188813",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "Train_Dataset = r'D:\\Rakesh\\Guvi\\project\\microsoft\\Train.csv'\n",
    "Test_Dataset = r'D:\\Rakesh\\Guvi\\project\\microsoft\\Test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "602172c0-9f82-4a98-bcbc-92a1b217766b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Few Rows of the Dataset:\n",
      "     Id  OrgId  IncidentId  AlertId  DetectorId  AlertTitle  Category  \\\n",
      "0  3028     75         478   939479           4    0.362809         6   \n",
      "1  1813      0         211   160158           0    0.358908         6   \n",
      "2  2390    206      251904  1009993           0    0.358908         6   \n",
      "3  1455     88       26726  1114326          58    0.414824         4   \n",
      "4  1731      0         375   169963           0    0.358908         6   \n",
      "\n",
      "   MitreTechniques  IncidentGrade  EntityType  ...  OSFamily  OSVersion  \\\n",
      "0               23              2           0  ...         0          0   \n",
      "1               23              2           0  ...         0          0   \n",
      "2               23              1           0  ...         0          0   \n",
      "3               23              1           7  ...         0          0   \n",
      "4               23              2           0  ...         0          0   \n",
      "\n",
      "   LastVerdict  CountryCode  State   City  Year     Month  DayOfWeek      Hour  \n",
      "0            2          242   1445  10630     1  0.454545   0.000000  0.608696  \n",
      "1            2          242   1445  10630     1  0.454545   0.166667  0.347826  \n",
      "2            2          242   1445  10630     1  0.454545   0.500000  0.695652  \n",
      "3            2          242   1445  10630     1  0.454545   0.000000  0.043478  \n",
      "4            2          242   1445  10630     1  0.454545   0.000000  0.217391  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "\n",
      "Shape of the dataset: (845110, 40)\n",
      "\n",
      "Data types of each column:\n",
      "Id                      int16\n",
      "OrgId                   int16\n",
      "IncidentId              int32\n",
      "AlertId                 int32\n",
      "DetectorId              int16\n",
      "AlertTitle            float64\n",
      "Category                 int8\n",
      "MitreTechniques          int8\n",
      "IncidentGrade            int8\n",
      "EntityType               int8\n",
      "EvidenceRole             int8\n",
      "DeviceId                int32\n",
      "Sha256                  int32\n",
      "IpAddress               int32\n",
      "Url                     int32\n",
      "AccountSid              int32\n",
      "AccountUpn              int32\n",
      "AccountObjectId         int32\n",
      "AccountName             int32\n",
      "DeviceName              int32\n",
      "NetworkMessageId        int32\n",
      "RegistryKey             int16\n",
      "RegistryValueName       int16\n",
      "RegistryValueData       int16\n",
      "ApplicationId           int16\n",
      "ApplicationName         int16\n",
      "OAuthApplicationId      int16\n",
      "FileName                int32\n",
      "FolderPath              int32\n",
      "ResourceIdName          int16\n",
      "OSFamily                 int8\n",
      "OSVersion                int8\n",
      "LastVerdict              int8\n",
      "CountryCode             int16\n",
      "State                   int16\n",
      "City                    int16\n",
      "Year                     int8\n",
      "Month                 float64\n",
      "DayOfWeek             float64\n",
      "Hour                  float64\n",
      "dtype: object\n",
      "Column Names in the Dataset:\n",
      "Index(['Id', 'OrgId', 'IncidentId', 'AlertId', 'DetectorId', 'AlertTitle',\n",
      "       'Category', 'MitreTechniques', 'IncidentGrade', 'EntityType',\n",
      "       'EvidenceRole', 'DeviceId', 'Sha256', 'IpAddress', 'Url', 'AccountSid',\n",
      "       'AccountUpn', 'AccountObjectId', 'AccountName', 'DeviceName',\n",
      "       'NetworkMessageId', 'RegistryKey', 'RegistryValueName',\n",
      "       'RegistryValueData', 'ApplicationId', 'ApplicationName',\n",
      "       'OAuthApplicationId', 'FileName', 'FolderPath', 'ResourceIdName',\n",
      "       'OSFamily', 'OSVersion', 'LastVerdict', 'CountryCode', 'State', 'City',\n",
      "       'Year', 'Month', 'DayOfWeek', 'Hour'],\n",
      "      dtype='object')\n",
      "\n",
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 845110 entries, 0 to 845109\n",
      "Data columns (total 40 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   Id                  845110 non-null  int16  \n",
      " 1   OrgId               845110 non-null  int16  \n",
      " 2   IncidentId          845110 non-null  int32  \n",
      " 3   AlertId             845110 non-null  int32  \n",
      " 4   DetectorId          845110 non-null  int16  \n",
      " 5   AlertTitle          845110 non-null  float64\n",
      " 6   Category            845110 non-null  int8   \n",
      " 7   MitreTechniques     845110 non-null  int8   \n",
      " 8   IncidentGrade       845110 non-null  int8   \n",
      " 9   EntityType          845110 non-null  int8   \n",
      " 10  EvidenceRole        845110 non-null  int8   \n",
      " 11  DeviceId            845110 non-null  int32  \n",
      " 12  Sha256              845110 non-null  int32  \n",
      " 13  IpAddress           845110 non-null  int32  \n",
      " 14  Url                 845110 non-null  int32  \n",
      " 15  AccountSid          845110 non-null  int32  \n",
      " 16  AccountUpn          845110 non-null  int32  \n",
      " 17  AccountObjectId     845110 non-null  int32  \n",
      " 18  AccountName         845110 non-null  int32  \n",
      " 19  DeviceName          845110 non-null  int32  \n",
      " 20  NetworkMessageId    845110 non-null  int32  \n",
      " 21  RegistryKey         845110 non-null  int16  \n",
      " 22  RegistryValueName   845110 non-null  int16  \n",
      " 23  RegistryValueData   845110 non-null  int16  \n",
      " 24  ApplicationId       845110 non-null  int16  \n",
      " 25  ApplicationName     845110 non-null  int16  \n",
      " 26  OAuthApplicationId  845110 non-null  int16  \n",
      " 27  FileName            845110 non-null  int32  \n",
      " 28  FolderPath          845110 non-null  int32  \n",
      " 29  ResourceIdName      845110 non-null  int16  \n",
      " 30  OSFamily            845110 non-null  int8   \n",
      " 31  OSVersion           845110 non-null  int8   \n",
      " 32  LastVerdict         845110 non-null  int8   \n",
      " 33  CountryCode         845110 non-null  int16  \n",
      " 34  State               845110 non-null  int16  \n",
      " 35  City                845110 non-null  int16  \n",
      " 36  Year                845110 non-null  int8   \n",
      " 37  Month               845110 non-null  float64\n",
      " 38  DayOfWeek           845110 non-null  float64\n",
      " 39  Hour                845110 non-null  float64\n",
      "dtypes: float64(4), int16(13), int32(14), int8(9)\n",
      "memory usage: 99.1 MB\n",
      "None\n",
      "\n",
      "Missing values:\n",
      "Id                    0\n",
      "OrgId                 0\n",
      "IncidentId            0\n",
      "AlertId               0\n",
      "DetectorId            0\n",
      "AlertTitle            0\n",
      "Category              0\n",
      "MitreTechniques       0\n",
      "IncidentGrade         0\n",
      "EntityType            0\n",
      "EvidenceRole          0\n",
      "DeviceId              0\n",
      "Sha256                0\n",
      "IpAddress             0\n",
      "Url                   0\n",
      "AccountSid            0\n",
      "AccountUpn            0\n",
      "AccountObjectId       0\n",
      "AccountName           0\n",
      "DeviceName            0\n",
      "NetworkMessageId      0\n",
      "RegistryKey           0\n",
      "RegistryValueName     0\n",
      "RegistryValueData     0\n",
      "ApplicationId         0\n",
      "ApplicationName       0\n",
      "OAuthApplicationId    0\n",
      "FileName              0\n",
      "FolderPath            0\n",
      "ResourceIdName        0\n",
      "OSFamily              0\n",
      "OSVersion             0\n",
      "LastVerdict           0\n",
      "CountryCode           0\n",
      "State                 0\n",
      "City                  0\n",
      "Year                  0\n",
      "Month                 0\n",
      "DayOfWeek             0\n",
      "Hour                  0\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows in the dataset: 0\n",
      "\n",
      "Target variable distribution:\n",
      "IncidentGrade\n",
      "2    658024\n",
      "1    101147\n",
      "0     85939\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load data in chunks to handle large file size\n",
    "chunk_size = 500000  # Adjust chunk size for memory efficiency\n",
    "chunks = []\n",
    "\n",
    "# Reading the dataset in chunks\n",
    "for chunk in pd.read_csv(Train_Dataset, chunksize=chunk_size, low_memory=False):\n",
    "    # Optimize memory usage by downcasting data types\n",
    "    for col in chunk.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        chunk[col] = pd.to_numeric(chunk[col], downcast='integer')\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free up memory\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First Few Rows of the Dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display the shape of the dataset\n",
    "print(\"\\nShape of the dataset:\", df.shape)\n",
    "\n",
    "# Display the data types of each column\n",
    "print(\"\\nData types of each column:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display the column names\n",
    "print(\"Column Names in the Dataset:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Display data types and non-null counts of each column\n",
    "print(\"\\nDataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(\"\\nNumber of duplicate rows in the dataset:\", duplicate_count)\n",
    "\n",
    "# Target variable distribution for the entire dataset\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "print(df['IncidentGrade'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "343e2329-1523-435b-ba95-7af250784231",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and validation splits created successfully.\n",
      "Training set shape: (67608, 39)\n",
      "Validation set shape: (16903, 39)\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv(Train_Dataset)\n",
    "\n",
    "# Specify your target variable\n",
    "target_variable = 'IncidentGrade'  # Change this to your target variable column name\n",
    "\n",
    "# Define the stratified sampling size (10% of the dataset)\n",
    "sampling_size = 0.10\n",
    "\n",
    "# Optionally, sample 10% of the dataset if needed\n",
    "if sampling_size > 0:\n",
    "    df = df.sample(frac=sampling_size, random_state=42)\n",
    "\n",
    "# Perform the train-validation split with stratification\n",
    "X = df.drop(columns=[target_variable])  # Features\n",
    "y = df[target_variable]  # Target\n",
    "\n",
    "# Split the dataset into training and validation sets with 80-20 split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.20,  # 80% train, 20% validation\n",
    "    stratify=y,      # Stratify based on the target variable\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Optional: Convert to DataFrames if needed\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "valid_df = pd.concat([X_valid, y_valid], axis=1)\n",
    "\n",
    "# Save the splits to CSV files (optional)\n",
    "train_df.to_csv(r'D:\\Rakesh\\Guvi\\project\\microsoft\\Train_split.csv', index=False)\n",
    "valid_df.to_csv(r'D:\\Rakesh\\Guvi\\project\\microsoft\\Valid_split.csv', index=False)\n",
    "\n",
    "print(\"Train and validation splits created successfully.\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5657000-cc9b-4c94-bca5-f34a577cab59",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters:\n",
      "{'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.61      0.60      1745\n",
      "           1       0.78      0.61      0.69      1995\n",
      "           2       0.92      0.95      0.94     13163\n",
      "\n",
      "    accuracy                           0.88     16903\n",
      "   macro avg       0.77      0.72      0.74     16903\n",
      "weighted avg       0.87      0.88      0.87     16903\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1063   187   495]\n",
      " [  218  1226   551]\n",
      " [  504   154 12505]]\n",
      "Training Time: 242.82 seconds\n",
      "Memory Usage: 496.73 MB\n"
     ]
    }
   ],
   "source": [
    "# Load stratified sampled data\n",
    "train_file_path = r'D:\\Rakesh\\Guvi\\project\\microsoft\\Train_split.csv'\n",
    "valid_file_path = r'D:\\Rakesh\\Guvi\\project\\microsoft\\Valid_split.csv'\n",
    "\n",
    "# Read the datasets\n",
    "df_train = pd.read_csv(train_file_path)\n",
    "df_valid = pd.read_csv(valid_file_path)\n",
    "\n",
    "# Prepare features and target variable\n",
    "X_train = df_train.drop(columns=['IncidentGrade'])\n",
    "y_train = df_train['IncidentGrade']\n",
    "X_valid = df_valid.drop(columns=['IncidentGrade'])\n",
    "y_valid = df_valid['IncidentGrade']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Initialize logistic regression\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']  # 'liblinear' is required for l1 penalty\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=logreg, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Train model with time tracking\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Get the best model\n",
    "best_logreg = grid_search.best_estimator_\n",
    "\n",
    "# Get memory usage\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_usage = process.memory_info().rss / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Evaluate the best model on validation data\n",
    "y_pred = best_logreg.predict(X_valid_scaled)\n",
    "\n",
    "# Print results\n",
    "print(\"Best Parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "print(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Memory Usage: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2ce811a-16ef-43a2-880d-f7f5221e9a00",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rakes\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:269: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'criterion': 'gini', 'max_depth': 15, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77      1745\n",
      "           1       0.88      0.85      0.86      1995\n",
      "           2       0.98      0.96      0.97     13163\n",
      "\n",
      "    accuracy                           0.94     16903\n",
      "   macro avg       0.86      0.88      0.87     16903\n",
      "weighted avg       0.94      0.94      0.94     16903\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1445   142   158]\n",
      " [  203  1686   106]\n",
      " [  383    94 12686]]\n",
      "Training Time: 6.33 seconds\n",
      "Memory Usage: 408.62 MB\n"
     ]
    }
   ],
   "source": [
    "# Read the datasets\n",
    "df_train = pd.read_csv(train_file_path)\n",
    "df_valid = pd.read_csv(valid_file_path)\n",
    "\n",
    "# Prepare features and target variable\n",
    "X_train = df_train.drop(columns=['IncidentGrade'])\n",
    "y_train = df_train['IncidentGrade']\n",
    "X_valid = df_valid.drop(columns=['IncidentGrade'])\n",
    "y_valid = df_valid['IncidentGrade']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Initialize decision tree classifier\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': randint(1, 20),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=decision_tree, param_distributions=param_dist, \n",
    "                                    n_iter=50, scoring='accuracy', cv=5, n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "# Train model with time tracking\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Get the best model\n",
    "best_decision_tree = random_search.best_estimator_\n",
    "\n",
    "# Get memory usage\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_usage = process.memory_info().rss / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Evaluate the best model on validation data\n",
    "y_pred = best_decision_tree.predict(X_valid_scaled)\n",
    "\n",
    "# Print results\n",
    "print(\"Best Parameters:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "print(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Memory Usage: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d813746-57d5-45d3-8cd6-bd6ec406943e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 30, 'bootstrap': True}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      1745\n",
      "           1       0.94      0.93      0.94      1995\n",
      "           2       0.99      0.99      0.99     13163\n",
      "\n",
      "    accuracy                           0.98     16903\n",
      "   macro avg       0.95      0.96      0.96     16903\n",
      "weighted avg       0.98      0.98      0.98     16903\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1653    64    28]\n",
      " [   69  1861    65]\n",
      " [   57    51 13055]]\n",
      "Training Time: 2757.43 seconds\n",
      "Memory Usage: 172.64 MB\n"
     ]
    }
   ],
   "source": [
    "# Load prepared data\n",
    "X_train = pd.read_csv(r'D:\\Rakesh\\Guvi\\project\\microsoft\\Train_split.csv')\n",
    "y_train = X_train.pop('IncidentGrade')\n",
    "X_val = pd.read_csv(r'D:\\Rakesh\\Guvi\\project\\microsoft\\Valid_split.csv')\n",
    "y_val = X_val.pop('IncidentGrade')\n",
    "\n",
    "# Initialize Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'max_features': ['sqrt', 'log2', None],  # Number of features to consider for the best split\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "# Setup randomized search with cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Number of parameter settings to sample\n",
    "    cv=cv,\n",
    "    scoring='f1_macro',  # Use F1 score for evaluation\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model with randomized search and time tracking\n",
    "start_time = time.time()\n",
    "\n",
    "# Use joblib for parallel processing\n",
    "with parallel_backend('loky'):\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Get memory usage\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_usage = process.memory_info().rss / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Get the best model\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "y_pred = best_rf.predict(X_val)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Hyperparameters: {random_search.best_params_}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Memory Usage: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f51f281e-6d80-4e10-bdfb-44621cf7fa8e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rakes\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:46:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'subsample': 1.0, 'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      1745\n",
      "           1       0.94      0.93      0.94      1995\n",
      "           2       0.99      0.99      0.99     13163\n",
      "\n",
      "    accuracy                           0.98     16903\n",
      "   macro avg       0.95      0.95      0.95     16903\n",
      "weighted avg       0.98      0.98      0.98     16903\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1644    62    39]\n",
      " [   76  1853    66]\n",
      " [   61    53 13049]]\n",
      "Training Time: 243.25 seconds\n",
      "Memory Usage: 460.03 MB\n"
     ]
    }
   ],
   "source": [
    "# Load prepared data\n",
    "X_train = pd.read_csv(r'D:\\Rakesh\\Guvi\\project\\microsoft\\Train_split.csv')\n",
    "y_train = X_train.pop('IncidentGrade')\n",
    "X_val = pd.read_csv(r'D:\\Rakesh\\Guvi\\project\\microsoft\\Valid_split.csv')\n",
    "y_val = X_val.pop('IncidentGrade')\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Define optimized parameter grid for randomized search\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of boosting rounds\n",
    "    'max_depth': [3, 6, 9],           # Maximum depth of a tree\n",
    "    'learning_rate': [0.01, 0.1, 0.2], # Step size shrinkage\n",
    "    'subsample': [0.7, 0.8, 1.0],     # Fraction of samples used for fitting each base learner\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0] # Fraction of features used for each tree\n",
    "}\n",
    "\n",
    "# Create a custom fit method with early stopping\n",
    "fit_with_early_stopping = partial(\n",
    "    xgb_model.fit,\n",
    "    early_stopping_rounds=10,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "xgb_model.fit = fit_with_early_stopping\n",
    "\n",
    "# Setup randomized search with cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=10,  # Number of parameter settings to sample\n",
    "    cv=cv, \n",
    "    scoring='f1_macro',  # Use F1 score for evaluation\n",
    "    n_jobs=-1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model with randomized search and time tracking\n",
    "start_time = time.time()\n",
    "\n",
    "# Use joblib for parallel processing\n",
    "with parallel_backend('loky'):\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Get memory usage\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_usage = process.memory_info().rss / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Get the best model\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "y_pred = best_xgb.predict(X_val)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Hyperparameters: {random_search.best_params_}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Memory Usage: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f94ded05-4a19-4fb5-a6f7-a03ab3fee622",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1431\n",
      "[LightGBM] [Info] Number of data points in the train set: 67608, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score -2.270821\n",
      "[LightGBM] [Info] Start training from score -2.136913\n",
      "[LightGBM] [Info] Start training from score -0.250060\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best Hyperparameters: {'subsample': 0.8, 'num_leaves': 70, 'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94      1745\n",
      "           1       0.94      0.93      0.93      1995\n",
      "           2       0.99      0.99      0.99     13163\n",
      "\n",
      "    accuracy                           0.98     16903\n",
      "   macro avg       0.96      0.95      0.96     16903\n",
      "weighted avg       0.98      0.98      0.98     16903\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1648    67    30]\n",
      " [   68  1851    76]\n",
      " [   49    50 13064]]\n",
      "Training Time: 261.64 seconds\n",
      "Memory Usage: 472.95 MB\n"
     ]
    }
   ],
   "source": [
    "# Load prepared data\n",
    "X_train = pd.read_csv(r'D:\\Rakesh\\Guvi\\project\\microsoft\\Train_split.csv')\n",
    "y_train = X_train.pop('IncidentGrade')\n",
    "X_val = pd.read_csv(r'D:\\Rakesh\\Guvi\\project\\microsoft\\Valid_split.csv')\n",
    "y_val = X_val.pop('IncidentGrade')\n",
    "\n",
    "# Initialize LightGBM classifier\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "# Define parameter grid for random search\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],          # Number of boosting rounds\n",
    "    'learning_rate': [0.01, 0.05, 0.1],       # Step size shrinkage\n",
    "    'num_leaves': [31, 50, 70],                # Maximum number of leaves in one tree\n",
    "    'max_depth': [-1, 10, 20],                 # Maximum depth of a tree (-1 means no limit)\n",
    "    'subsample': [0.7, 0.8, 1.0],             # Fraction of samples used for fitting each base learner\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]       # Fraction of features used for each tree\n",
    "}\n",
    "\n",
    "# Setup random search with cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    lgbm,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Number of parameter settings to sample\n",
    "    cv=cv,\n",
    "    scoring='f1_macro',  # Use F1 score for evaluation\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model with random search and time tracking\n",
    "start_time = time.time()\n",
    "with parallel_backend('loky'):\n",
    "    random_search.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Get memory usage\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_usage = process.memory_info().rss / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Get the best model\n",
    "best_lgbm = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "y_pred = best_lgbm.predict(X_val)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Hyperparameters: {random_search.best_params_}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Memory Usage: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7326ec93-258a-421d-9e43-6090feeb089c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rakes\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2113/2113 - 14s - 7ms/step - accuracy: 0.8293 - loss: 0.4808 - val_accuracy: 0.8555 - val_loss: 0.4086 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "2113/2113 - 7s - 3ms/step - accuracy: 0.8579 - loss: 0.3731 - val_accuracy: 0.6429 - val_loss: 0.9235 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "2113/2113 - 7s - 3ms/step - accuracy: 0.8627 - loss: 0.3532 - val_accuracy: 0.3628 - val_loss: 1.0887 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "2113/2113 - 11s - 5ms/step - accuracy: 0.8658 - loss: 0.3437 - val_accuracy: 0.8483 - val_loss: 1.1220 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "2113/2113 - 7s - 3ms/step - accuracy: 0.8701 - loss: 0.3278 - val_accuracy: 0.8813 - val_loss: 0.3612 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "2113/2113 - 7s - 3ms/step - accuracy: 0.8713 - loss: 0.3234 - val_accuracy: 0.3783 - val_loss: 1.6693 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "2113/2113 - 7s - 3ms/step - accuracy: 0.8731 - loss: 0.3193 - val_accuracy: 0.8802 - val_loss: 0.2912 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "2113/2113 - 7s - 3ms/step - accuracy: 0.8738 - loss: 0.3143 - val_accuracy: 0.6314 - val_loss: 0.7933 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "2113/2113 - 7s - 3ms/step - accuracy: 0.8718 - loss: 0.3135 - val_accuracy: 0.7988 - val_loss: 3.4193 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "2113/2113 - 7s - 4ms/step - accuracy: 0.8729 - loss: 0.3149 - val_accuracy: 0.8755 - val_loss: 0.5008 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "2113/2113 - 7s - 4ms/step - accuracy: 0.8748 - loss: 0.3072 - val_accuracy: 0.6853 - val_loss: 0.8687 - learning_rate: 2.5000e-04\n",
      "Epoch 12/50\n",
      "2113/2113 - 7s - 3ms/step - accuracy: 0.8751 - loss: 0.3040 - val_accuracy: 0.5213 - val_loss: 0.7944 - learning_rate: 2.5000e-04\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.61      0.58      1745\n",
      "           1       0.74      0.77      0.76      1995\n",
      "           2       0.95      0.93      0.94     13163\n",
      "\n",
      "    accuracy                           0.88     16903\n",
      "   macro avg       0.75      0.77      0.76     16903\n",
      "weighted avg       0.89      0.88      0.88     16903\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1062   281   402]\n",
      " [  244  1546   205]\n",
      " [  629   264 12270]]\n",
      "Training Time: 96.98 seconds\n",
      "Memory Usage: 540.29 MB\n"
     ]
    }
   ],
   "source": [
    "# Load prepared data\n",
    "X_train = pd.read_csv(r'D:\\Rakesh\\Guvi\\project\\microsoft\\Train_split.csv')\n",
    "y_train = X_train.pop('IncidentGrade')\n",
    "X_val = pd.read_csv(r'D:\\Rakesh\\Guvi\\project\\microsoft\\Valid_split.csv')\n",
    "y_val = X_val.pop('IncidentGrade')\n",
    "\n",
    "# One-hot encoding if needed\n",
    "y_train = pd.get_dummies(y_train).values\n",
    "y_val = pd.get_dummies(y_val).values\n",
    "\n",
    "# Initialize a neural network for multiclass classification\n",
    "def create_model(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Softmax activation for multiclass classification\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "num_classes = y_train.shape[1]\n",
    "model = create_model(X_train.shape[1], num_classes)\n",
    "\n",
    "# Set up early stopping and learning rate reduction on plateau\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n",
    "\n",
    "# Train the model with early stopping and time tracking\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,  # Start with fewer epochs\n",
    "    batch_size=32,  # Smaller batch size for quicker updates\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# Get memory usage\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_usage = process.memory_info().rss / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_classes = y_pred.argmax(axis=-1)\n",
    "y_val_classes = y_val.argmax(axis=-1)\n",
    "\n",
    "# Print results\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val_classes, y_pred_classes))\n",
    "print(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Memory Usage: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4bbd70-508e-4e1f-97b4-568c0d4d7244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
